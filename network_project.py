# -*- coding: utf-8 -*-
"""Network Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ZjVg4cb1oca3WrgAuAIhechITcPPpjC

#**<h1>Preprocessing</h1>**
"""

from google.colab import files
uploaded = files.upload()

!git clone https://github.com/dgunter/ParseZeekLogs.git
!git clone https://github.com/elastic/elasticsearch-py.git
!git clone https://github.com/elastic/elastic-transport-python.git
!git clone https://github.com/yzhao062/pyod.git

import sys
sys.path.insert(0,'/content/ParseZeekLogs')
sys.path.insert(0,'/content/elasticsearch-py')
sys.path.insert(0,'/content/elastic-transport-python')
sys.path.insert(0,'/content/pyod')

from parsezeeklogs import ParseZeekLogs

with open('out.json',"w") as outfile:
    for log_record in ParseZeekLogs("conn.log", output_format="json", safe_headers=False):
        if log_record is not None:
            outfile.write(log_record + "\n")

import pandas as pd

data = pd.read_json('/content/out.json', lines=True)

data.head()

data.info()

"""#**<h1>Graph Preparation<h2>**"""

src = data['id.orig_h'].tolist()
dist = data['id.resp_h'].tolist()

nodes = list(set(src + dist))
edges = list(zip(src, dist))  # repeated edges not unique
connection_count = {}
for edge in edges: connection_count[edge] = 0
for edge in edges: connection_count[edge] += 1
edges = list(set(edges))      # make edges unique

len(connection_count)

len(edges)

len(nodes)

nodes

edges

import networkx as nx

DG = nx.DiGraph()
DG.add_nodes_from(nodes)
DG.add_edges_from(edges)
for edge in edges:
  DG[edge[0]][edge[1]]['weight'] = connection_count[edge]

for edge in edges:
  print(edge, DG[edge[0]][edge[1]]['weight'])

nx.draw(DG, with_labels=True, font_weight='bold')

"""#**<h1>Feature Generation</h1>**"""

in_degree = nx.in_degree_centrality(DG)

out_degree = nx.out_degree_centrality(DG)

katz = nx.katz_centrality(DG)

betweenness = nx.betweenness_centrality(DG)

closeness = nx.closeness_centrality(DG)

"""# **<h1>Understanding Features</h1>**"""

# (in_degree of a node) divided by (total number of nodes-1)
in_degree

# (out_degree of a node) divided by (total number of nodes-1)
out_degree

# Katz centrality computes the relative influence of a node within a network by measuring the number
# of the mmediate neighbors (first degree nodes) and also all other nodes in the network that connect
# to the node under consideration through these immediate neighbors.
#-----------------------------------------------------------------
# Katz broadcast centrality captures the behavior of spreading a rumor into the network and
# a high value of Katz broadcast centrality means that the given node is efficient in spreading out
# a rumor/marketing message into the network.
katz

# Betweenness centrality of a node u is the sum of the fraction of all-pairs shortest paths
# that pass through u
#-----------------------------------------------------------------
# We compute all-pairs shortest paths in the component where u resides
# and see how much of these shortes-paths goes through u
betweenness

# Closeness centrality of a node u is the reciprocal of the average shortest path distance
# to u over all n-1 reachable nodes.
#----------------------------------------------------------------------
# shortes_paths_sum = summation of all shortest paths form node u to all other nodes in the same graph component
# n-1 = number of nodes in the graph component where u exists (excluding u)
# closeness = (n-1)/(shortes_paths_sum) 
#----------------------------------------------------------------------
# Notice that higher values of closeness indicate higher centrality.
#----------------------------------------------------------------------
# How much I'm close to the nodes in my component (in terms of shortest paths)
closeness

"""#**<h1>Generating Feature Vector</h1>**"""

feature_vector = {}

feature_vector = {'Node': list(in_degree.keys()),
                  'In_Degree': list(in_degree.values()),
                  'Out_Degree': list(out_degree.values()),
                  'Katz': list(katz.values()),
                  'Betweenness': list(betweenness.values()),
                  'Closeness': list(closeness.values())
                  }

data = pd.DataFrame.from_dict(feature_vector)

data.set_index('Node', inplace=True)

"""#**<h1>Preliminary investigation</h1>**"""

data.head()

data.tail()

data.info()

data.describe()

"""#**<h1>Univariate Anomaly Detection</h1>**

*Since we have just five features we will do Univariate Anomaly Detection on them all to extract some insights that could be useful to us while doing our Multivariate Anomaly Detection.*
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from sklearn.ensemble import IsolationForest

"""**<h1>Univariate_analyzer</h1>**"""

class Univariate_analyzer():
  def __init__(self, feature_name):
    self.feature_name = feature_name

  def feature_histogram(self, range=(-0.1, 0.5)):
    x = data[self.feature_name].to_numpy()
    plt.hist(x)
    plt.xlim([range[0], range[1]])
    plt.show()

  def feature_scatter(self):
    plt.scatter(data.index, data[self.feature_name])
    plt.xlabel('IP addresse')
    plt.ylabel(self.feature_name)
    plt.title(self.feature_name + " distribution")
    sns.despine()

  def feature_distribution(self):
    sns.distplot(data[self.feature_name])
    plt.title("Distribution of " + self.feature_name)
    sns.despine()

  def feature_skewKurt(self):
    print("Skewness: %f" % data[self.feature_name].skew())
    print("Kurtosis: %f" % data[self.feature_name].kurt())

  def feature_isolationforest(self):
    isolation_forest = IsolationForest(n_estimators=100)
    isolation_forest.fit(data[self.feature_name].values.reshape(-1, 1))
    xx = np.linspace(data[self.feature_name].min(), data[self.feature_name].max(), len(data)).reshape(-1,1)
    anomaly_score = isolation_forest.decision_function(xx)
    outlier = isolation_forest.predict(xx)
    plt.figure(figsize=(10,4))
    plt.plot(xx, anomaly_score, label='anomaly score')
    plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), 
                     where=outlier==-1, color='r', 
                     alpha=.4, label='outlier region')
    plt.legend()
    plt.ylabel('anomaly score')
    plt.xlabel(self.feature_name)
    plt.show();

  def feature_anomalies_IPs(self):
    '''
    This functions takes one feature (column) from our dataframe
    and returns the top 20 anomalies IP addresses based on the mean difference
    '''
    mean_val = data[self.feature_name].mean()
    mean_diff = []
    feature_Anomalies = [] # IP addresses
  
    for idx in range(len(data)):
     mean_diff.append( (abs(data[self.feature_name][idx]-mean_val), idx) )
  
    mean_diff.sort(reverse=True)
  
    for idx in range(20):
      feature_Anomalies.append(data[self.feature_name].index[mean_diff[idx][1]])
  
    return feature_Anomalies

"""##**<h1>In_Degree Feature</h1>**"""

in_degree_analyzer = Univariate_analyzer('In_Degree')

in_degree_analyzer.feature_histogram(range=(-0.08, 0.08))

in_degree_analyzer.feature_scatter()

"""*We can see that the IP addresses having In_Degree value of 0.06 and 0.04 are outliers*"""

in_degree_analyzer.feature_distribution()

#Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks
#the same to the left and right of the center point. Kurtosis is a measure of whether the data are heavy-tailed or light-tailed 
#relative to a normal distribution.

in_degree_analyzer.feature_skewKurt()

""" *Our In_Degree distribution is not that far from a normal distribution, and it has a long tail from the right side. All the previous digrams clearly shows that the Probability of an IP address having In_Degree value between 0.005 and 0.025 is very high, in other words most of our IP address has In_Degree value between 0.005 and 0.025 and outside that range outliers resides, we can check this results by running the Isolation Forest model to confirm our conclusions*"""

in_degree_analyzer.feature_isolationforest()

"""***While going on with our Univariate Anomaly Detection we will compute a list containing the top 20 outliers for each feature based on the mean difference. We can compute the intersections of these lists. The intersection of these lists (or some of them) may give us some indication about the over all outliers in the network and could also verify our results when we use  a Univariate Anomaly Detection algorithm, moreover we can use these intersections as labels in our semi-supervised model***"""

In_Degree_Anomalies = in_degree_analyzer.feature_anomalies_IPs()

In_Degree_Anomalies

"""##**<h1>Out_Degree Feature</h1>**"""

out_degree_analyzer = Univariate_analyzer('Out_Degree')

out_degree_analyzer.feature_histogram(range=(-0.1, 0.4))

out_degree_analyzer.feature_scatter()

out_degree_analyzer.feature_distribution()

out_degree_analyzer.feature_skewKurt()

out_degree_analyzer.feature_isolationforest()

Out_Degree_Anomalies = out_degree_analyzer.feature_anomalies_IPs()

Out_Degree_Anomalies

"""##**<h1>Katz Feature</h1>**"""

katz_analyzer = Univariate_analyzer('Katz')

katz_analyzer.feature_histogram(range=(0.0, 0.2))

katz_analyzer.feature_scatter()

katz_analyzer.feature_distribution()

katz_analyzer.feature_skewKurt()

katz_analyzer.feature_isolationforest()

katz_Anomalies = katz_analyzer.feature_anomalies_IPs()

katz_Anomalies

"""##**<h1>Closeness Feature</h1>**"""

closeness_analyzer = Univariate_analyzer('Closeness')

closeness_analyzer.feature_histogram(range=(-0.03, 0.08))

closeness_analyzer.feature_scatter()

closeness_analyzer.feature_distribution()

closeness_analyzer.feature_skewKurt()

closeness_analyzer.feature_isolationforest()

Closeness_Anomalies = closeness_analyzer.feature_anomalies_IPs()

Closeness_Anomalies

"""##**<h1>Betweenness Feature</h1>**"""

betweenness_analyzer = Univariate_analyzer('Betweenness')

betweenness_analyzer.feature_histogram(range=(-0.01, 0.03))

betweenness_analyzer.feature_scatter()

betweenness_analyzer.feature_distribution()

betweenness_analyzer.feature_skewKurt()

betweenness_analyzer.feature_isolationforest()

Betweenness_Anomalies = betweenness_analyzer.feature_anomalies_IPs()

Betweenness_Anomalies

a = set(In_Degree_Anomalies[:5])
b = set(Out_Degree_Anomalies[:5])
c = set(Betweenness_Anomalies[:5])
d = set(Closeness_Anomalies[:5])
e = set(katz_Anomalies[:5])

# check intersections

"""#**<h1>Multivariate Anomaly Detection</h1>**"""

